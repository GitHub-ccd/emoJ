{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](./ppt/myLogo.png) emoJ - A Journal of Emotion\n",
    "\n",
    "\n",
    "# Vision:\n",
    "------------\n",
    "Socrates said **“To know thyself is the beginning of wisdom”**. Thus, we use journals, blogs, vlogs and other tools to record and track ourselves to know ourselves better. As the saying goes **“A picture is worth a 1000 words”**, and detecting human emotion from a picture or a video feed is one of the most active fields in computer vision today.  We can leverage the computer vision technology to evaluate, record and track our own emotional state in our daily lives. Therefore, I've built an app that can train, re-train CNN with the use of transfer learning to record and track one's own emotion in an emotion-journal or an e-journal if you will. \n",
    "World's foremost expert on facial expressions of humans, **Paul Ekman**, PhD have identified 7 base emotions that are well identified by facial expressions. These are **angry, disgust, fear, happy, sad, surprise, neutral**. \n",
    "\n",
    "![](./ppt/chandler.png)  \n",
    "\n",
    "# Data\n",
    "------------\n",
    "Researched on the image and video emotional recognition technology and the datasets to benchmark such technology. Downloaded the kaggel facial emotion recognition dataset (**FER2013**, about 35,888 images with size 28x28). To compliment the initial dataset google images were scraped. The final stage of the model will be continually updated and trained for the specific user. Thus the overall effectiveness of the CNN will grow with usage for a given user.\n",
    "\n",
    "The FER2013 dataset is published as a .csv files and images must be extracted from the flattened pixel matrix therein. Thus, I have followed the convention to handle all images in the project in .csv files. The scraped images after EDA is converted to string of pixels and stored in .csv file. Similarly this method is used to handle images in the Django app as well. This convention is most effecient for the Django app. \n",
    "\n",
    "\n",
    "## ROSEMED Methodology \n",
    "------------\n",
    "R - Research O - Obtain S - Scrub E - Explore M - Model E - Evaluate D - Deploy  \n",
    "\n",
    "During this process,the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc. It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of the frameworks, this methodology is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.  \n",
    "\n",
    "\n",
    "# Plan\n",
    "------------\n",
    "The overall strategy is a three stage process. \n",
    "1. FER2013 dataset is loaded and used to build the initial model. \n",
    "2. Scraped images and carefully labelled and used to further refine model built in stage-1\n",
    "3. Small set of images from a single user is gathered and trained to build the 3rd stage of the model, that will be a continuous improvement with usage. \n",
    "\n",
    "\n",
    "# Steps to build a emotion recongnition model at each stage\n",
    "------------\n",
    "Building a model at each stage givien above a 3 fold\n",
    "1. Using OpenCV [\"HAAR CASCADES\"](https://github.com/opencv/opencv/tree/master/data/haarcascades) face identification technology faces are identified especially in stages 2 and 3. Only pictures with single face is retained. Then a bounding box of the face is identified and the face alone is croped. \n",
    "2. The selected images from step 2 are visually inspected and outliers and incorrect images are removed. \n",
    "3. A CNN model is built using Keras library. \n",
    "\n",
    "\n",
    "# Dependencies\n",
    "------------\n",
    "Following dependencies are required to run this app:\n",
    "\n",
    "- numpy, pandas, Selenium\n",
    "- tensorflow, Keras\n",
    "- opencv-python\n",
    "- Django, pillow, sqlite3\n",
    "\n",
    "\n",
    "# Project Outline\n",
    "------------\n",
    "\n",
    "     ├── README.md                    <- Project description (this file)\n",
    "     │ \n",
    "     ├── emoJ/                        <- Django webapp (manage.py) \n",
    "     │   ├── emoj/                    <- Django app main organization code \n",
    "     │   │                               added secrets.py to hide the private key \n",
    "     │   ├── entries/                 <- code for handling Index and Form update \n",
    "     │   │   ├── models.py            <- create the database\n",
    "     │   │   ├── urls.py              <- add the urls to urlpatterns\n",
    "     │   │   ├── views.py\t         <- class to push data to html templates\n",
    "     │   │   ├── forms.py             <- class to create input form in app.html\n",
    "     │   │   └── FER.py \t\t\t     <- CNN inferance to predict emotion and pass \n",
    "     │   │                               it to views.py\n",
    "     │   ├── manage.py                <- run server           \n",
    "     │   ├── media/                   <- temperory images for CNN inferance, other static files.    \n",
    "     │   └── templates/               <- web templates and model files. \n",
    "     │       │                           (improved models \".h5\" can be set here)\n",
    "     │       └── entries              <- index.html and app.html                        \n",
    "     ├── util/                        <- utility scripts for web scraping and EDA\n",
    "     │   ├── search_and_download.py   <- Webscrping images from google via selenium\n",
    "     │   ├── face_crop.py             <- face detection, resize and crop images \n",
    "     │   └── face_2_num.py            <- convert croped and cleaned images to a .csv                       \n",
    "     ├── ppt/                         <- presentation and reports                        \n",
    "     ├── FED_regularize.ipynb         <- CNN model based on FER2013 dataset                       \n",
    "     ├── FED_transfer.ipynb           <- CNN model transfered from FER2013 and \n",
    "     │                                   re-trained on web scraped images dataset \n",
    "     ├── FIW_stage2.ipynb             <- CNN trained on FER2013+FIW datasets \n",
    "     ├── user_stage3.ipynb            <- CNN trained on small smaple of single user   \n",
    "     └── models/                      <- saved model .h5 files. (too large files were removed)\n",
    "\n",
    "\n",
    "# Findings:\n",
    "------------\n",
    "\n",
    "Model|Loss|Accuracy|Val_Loss|Val_Accuracy|\n",
    "-----|----|--------|--------|------------|\n",
    "Base Model|0.06|0.97|3.10|0.56|\n",
    "Improved Model|0.02|0.76|0.01|0.61|             \n",
    "\n",
    "(__for greater accuracy longer epoc is needed__)\n",
    "\n",
    "![](./ppt/cm.png)\n",
    "\n",
    "\n",
    "# predictions \n",
    "For any given image, the app will identify the main face, crop it and then make prediction of the emotional state as a bar chart. \n",
    "\n",
    "![](./ppt/emo_tests.jpg)  \n",
    "\n",
    "\n",
    "# Website:\n",
    "Following is the snapshots of the website for the emoJ webapp. \n",
    "\n",
    "![](./ppt/website_index.PNG) \n",
    "![](./ppt/website_form.png) \n",
    "\n",
    "\n",
    "# Conclusion:  \n",
    "------------\n",
    "This study presents a deep CNN based approach for the detection of human emotion from images. Our improved model has a 76% training and 61% validation accuracy. Using web scraped images proved to decrease the model accuracy. However, with a very small set of images from a single user can quickly build a model that is well optimized for that particular user. Thus, for the purpose of a journal app the models constructed here suffice. With continious improvement of the model will bring about higher accuracy for each individual user. \n",
    "\n",
    "\n",
    "# Recommendations:  \n",
    "------------\n",
    "- The model is demonstrated to work as a web app. \n",
    "- User must input at least 10 images per emotion to get a baseline model and give a reasonable accuracy\n",
    "- The app will be built to continuously feed new images and labels from the user so that it will improve predictions \n",
    "  with the usage.\n",
    "- The aggregation of journal entries will provide much valuable insight to users emotional state with time and could possible be an asset to psychiatrists, psychotherapist as well as general physicians as an informative tool to diagnose patients. \n",
    "\n",
    "\n",
    "# Future Work:\n",
    "------------\n",
    "- Implement user authentication and expand DB \n",
    "- add retraining model for each user \n",
    "- add capability for users to update labels so that model gain accuracy with usage\n",
    "- deploy the Django webapp \n",
    "- develop and deploy Android and IOS apps \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLweb]",
   "language": "python",
   "name": "conda-env-MLweb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
